{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############\n",
      "data loaded - head:\n",
      "Row(id=149, product_uid=100028, product_title=u'Backyard X-Scapes 6 ft. H. x 16 ft. L Reed Fencing', search_term=u'balcony privacy screen', relevance=2.67)\n",
      "################\n",
      "attributes loaded - head:\n",
      "Row(product_uid=100002, name=u'Opacity', value=u'Solid')\n",
      "################\n",
      "description loaded - head:\n",
      "Row(product_uid=100040, product_description=u'With an exceptional variety of different styles, Veranda Pro Series vinyl privacy fence kit is perfect for the pro or the do-it-yourselfer. Our vinyl fence offers the perfect combination of high quality and low maintenance you have been looking for. The lightweight kit components make installation fast and easy. The Washington Pro Series Privacy fence panel kit allows you to easily add privacy and curb appeal to your home.Corresponding posts sold separately, use: Line post (Model #73014884), corner post (Model #73014886), end post (Model #73014885)5 in. x 5 in. post tops (Various styles available) attach easiest by using Veranda post top clips (Model #73014080)Vinyl privacy fence assembles in minutes without the use of brackets, fasteners or screwsCoordinating 4 ft. x 4 ft. walk gate (Model # 73014853), 4 ft. x 5 ft. drive gate (Model # 73014854)Tongue and groove boards fit together with no gap in between for true privacy in your yardPro series vinyl privacy fence kits contour to follow the slopes of your yardMade of durable, low maintenance vinylIncludes a transferable limited lifetime warranty')\n",
      "################\n",
      "new RDD:\n",
      "(100002, ['Opacity Solid.'])\n",
      "################\n",
      "Aggregated by product_id:\n",
      "(204800, \"Bullet04 Insulated. HVAC application Heating. MFG Brand Name Williams. Product Depth (in.) 2. Product Height (in.) 35. Product Weight (lb.) 11. Product Width (in.) 37. Brand/Model Compatibility Williams Room Console Heaters - 50,000 and 65,000 Btu. Bullet01 Powder-coated black finish. Bullet02 For use with Williams' room console heater models 5001521, 5001522, 5001921, 5001922, 5002521, 5002522, 5002921, 5002922, 6501521, 6501522, 6501921, 6501922, 6502521, 6502522, 6502921 and 6502922. Bullet03 For use when your Williams' heater is placed on carpeting or vinyl-type flooring.\")\n",
      "################\n",
      "New dataframe from aggregated attributes:\n",
      "Row(product_uid=204800, attributes=u\"Bullet04 Insulated. HVAC application Heating. MFG Brand Name Williams. Product Depth (in.) 2. Product Height (in.) 35. Product Weight (lb.) 11. Product Width (in.) 37. Brand/Model Compatibility Williams Room Console Heaters - 50,000 and 65,000 Btu. Bullet01 Powder-coated black finish. Bullet02 For use with Williams' room console heater models 5001521, 5001522, 5001921, 5001922, 5002521, 5002522, 5002921, 5002922, 6501521, 6501522, 6501921, 6501922, 6502521, 6502522, 6502921 and 6502922. Bullet03 For use when your Williams' heater is placed on carpeting or vinyl-type flooring.\")\n",
      "################\n",
      "Joined Data:\n",
      "Row(product_uid=100501, id=2847, product_title=u'Ring Wireless Video Door Bell', search_term=u'door bell', relevance=3.0, attributes=u\"Adjustable Volume Yes. Bell Button Color Family Gray. Bell Wire Required Wireless. Bullet01 See and speak with visitors using your Smartphone or tablet, whether you're upstairs or across town. Bullet02 Compatible with all iOS and android Smartphone and tablets. Bullet03 Built-in motion sensors detect movement up to 30 ft. allowing you to know what is going on outside of your home. Bullet04 Multiple faceplate finishes helping you match your current door hardware. Bullet05 Connect to current doorbell wiring or utilize internal battery for convenience. Certifications and Listings No Certifications or Listings. Digital Bell No. Door Bell Or Intercom Type Door Bells. Door Chime Kit Type Wired With Contacts. Electrical Product Type Door Chime Kit. Mechanical Bell No. MFG Brand Name Ring. Multiple Songs No. Number of Buttons Included 2. Number of Sounds 1. Product Depth (in.) .9. Product Height (in.) 5. Product Width (in.) 2.4. Style Contemporary. Transformer Not Included. Westminster Bell No. Zone-specific Sounds No.\")\n",
      "################\n",
      "new RDD:\n",
      "Row(product_uid=100040, product_description=u'With an exceptional variety of different styles, Veranda Pro Series vinyl privacy fence kit is perfect for the pro or the do-it-yourselfer. Our vinyl fence offers the perfect combination of high quality and low maintenance you have been looking for. The lightweight kit components make installation fast and easy. The Washington Pro Series Privacy fence panel kit allows you to easily add privacy and curb appeal to your home.Corresponding posts sold separately, use: Line post (Model #73014884), corner post (Model #73014886), end post (Model #73014885)5 in. x 5 in. post tops (Various styles available) attach easiest by using Veranda post top clips (Model #73014080)Vinyl privacy fence assembles in minutes without the use of brackets, fasteners or screwsCoordinating 4 ft. x 4 ft. walk gate (Model # 73014853), 4 ft. x 5 ft. drive gate (Model # 73014854)Tongue and groove boards fit together with no gap in between for true privacy in your yardPro series vinyl privacy fence kits contour to follow the slopes of your yardMade of durable, low maintenance vinylIncludes a transferable limited lifetime warranty')\n",
      "################\n",
      "Joined Data:\n",
      "Row(product_uid=100170, id=1019, product_title=u'Dyna-Glo Pro 125,000 BTU Forced Air LP Gas Portable Heater', search_term=u'kerosene heater', relevance=1.0, attributes=u'Area Heated (Sq. Ft.) 3100. Bullet01 70,000 - 125,000 BTUs. Bullet02 Heats up to 3,100 sq. ft.. Bullet03 Continuously variable BTUs. Bullet04 Continuos electronic ignition - prevents dangerous delayed ignition. Certifications and Listings CSA Listed. Color Orange. Color Family Oranges / Peaches. Fuel rate (gallons/hour) 0. Fuel tank capacity (gallons) 0. Heat rating (BTU/hour) 125000. Heater Type Vented. Heating Product Type Gas Portable Heater. Heating Technology Type Convection. Ignition Type Continuous Spark. Indoor/Outdoor Indoor/Outdoor. Material Steel. MFG Brand Name Dyna-Glo Pro. Portable Heater Features Automatic Shutoff,Tip-Over Safety Switch,Wheels. Power/Fuel Type Propane. Product Depth (in.) 11.81. Product Height (in.) 16.14. Product Weight (lb.) 20.72. Product Width (in.) 25.59. Run time (hours) 12.', product_description=u'Dyna-Glo Pro portable gas forced air heaters offer a quick fire continuous spark ignition systems, coupled with adjustable height controls to insure you get the heat you need, when you need it, and where you want it. Fueled by liquid propane, this unit heats up to 3,100 sq. ft. 10 ft. Hose and regulator assembly are included (LP tank sold separately).70,000 - 125,000 BTUsHeats up to 3,100 sq. ft.Continuously variable BTUsContinuos electronic ignition - prevents dangerous delayed ignitionHome Depot Protection Plan:')\n",
      "################\n",
      "Tokenized Title:\n",
      "Row(product_uid=100170, id=1019, product_title=u'Dyna-Glo Pro 125,000 BTU Forced Air LP Gas Portable Heater', search_term=u'kerosene heater', relevance=1.0, attributes=u'Area Heated (Sq. Ft.) 3100. Bullet01 70,000 - 125,000 BTUs. Bullet02 Heats up to 3,100 sq. ft.. Bullet03 Continuously variable BTUs. Bullet04 Continuos electronic ignition - prevents dangerous delayed ignition. Certifications and Listings CSA Listed. Color Orange. Color Family Oranges / Peaches. Fuel rate (gallons/hour) 0. Fuel tank capacity (gallons) 0. Heat rating (BTU/hour) 125000. Heater Type Vented. Heating Product Type Gas Portable Heater. Heating Technology Type Convection. Ignition Type Continuous Spark. Indoor/Outdoor Indoor/Outdoor. Material Steel. MFG Brand Name Dyna-Glo Pro. Portable Heater Features Automatic Shutoff,Tip-Over Safety Switch,Wheels. Power/Fuel Type Propane. Product Depth (in.) 11.81. Product Height (in.) 16.14. Product Weight (lb.) 20.72. Product Width (in.) 25.59. Run time (hours) 12.', product_description=u'Dyna-Glo Pro portable gas forced air heaters offer a quick fire continuous spark ignition systems, coupled with adjustable height controls to insure you get the heat you need, when you need it, and where you want it. Fueled by liquid propane, this unit heats up to 3,100 sq. ft. 10 ft. Hose and regulator assembly are included (LP tank sold separately).70,000 - 125,000 BTUsHeats up to 3,100 sq. ft.Continuously variable BTUsContinuos electronic ignition - prevents dangerous delayed ignitionHome Depot Protection Plan:', words_title=[u'dyna-glo', u'pro', u'125,000', u'btu', u'forced', u'air', u'lp', u'gas', u'portable', u'heater'])\n",
      "################\n",
      "Tokenized Description:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(product_uid=100170, id=1019, product_title=u'Dyna-Glo Pro 125,000 BTU Forced Air LP Gas Portable Heater', search_term=u'kerosene heater', relevance=1.0, attributes=u'Area Heated (Sq. Ft.) 3100. Bullet01 70,000 - 125,000 BTUs. Bullet02 Heats up to 3,100 sq. ft.. Bullet03 Continuously variable BTUs. Bullet04 Continuos electronic ignition - prevents dangerous delayed ignition. Certifications and Listings CSA Listed. Color Orange. Color Family Oranges / Peaches. Fuel rate (gallons/hour) 0. Fuel tank capacity (gallons) 0. Heat rating (BTU/hour) 125000. Heater Type Vented. Heating Product Type Gas Portable Heater. Heating Technology Type Convection. Ignition Type Continuous Spark. Indoor/Outdoor Indoor/Outdoor. Material Steel. MFG Brand Name Dyna-Glo Pro. Portable Heater Features Automatic Shutoff,Tip-Over Safety Switch,Wheels. Power/Fuel Type Propane. Product Depth (in.) 11.81. Product Height (in.) 16.14. Product Weight (lb.) 20.72. Product Width (in.) 25.59. Run time (hours) 12.', product_description=u'Dyna-Glo Pro portable gas forced air heaters offer a quick fire continuous spark ignition systems, coupled with adjustable height controls to insure you get the heat you need, when you need it, and where you want it. Fueled by liquid propane, this unit heats up to 3,100 sq. ft. 10 ft. Hose and regulator assembly are included (LP tank sold separately).70,000 - 125,000 BTUsHeats up to 3,100 sq. ft.Continuously variable BTUsContinuos electronic ignition - prevents dangerous delayed ignitionHome Depot Protection Plan:', words_title=[u'dyna-glo', u'pro', u'125,000', u'btu', u'forced', u'air', u'lp', u'gas', u'portable', u'heater'], words_desc=[u'dyna-glo', u'pro', u'portable', u'gas', u'forced', u'air', u'heaters', u'offer', u'a', u'quick', u'fire', u'continuous', u'spark', u'ignition', u'systems,', u'coupled', u'with', u'adjustable', u'height', u'controls', u'to', u'insure', u'you', u'get', u'the', u'heat', u'you', u'need,', u'when', u'you', u'need', u'it,', u'and', u'where', u'you', u'want', u'it.', u'fueled', u'by', u'liquid', u'propane,', u'this', u'unit', u'heats', u'up', u'to', u'3,100', u'sq.', u'ft.', u'10', u'ft.', u'hose', u'and', u'regulator', u'assembly', u'are', u'included', u'(lp', u'tank', u'sold', u'separately).70,000', u'-', u'125,000', u'btusheats', u'up', u'to', u'3,100', u'sq.', u'ft.continuously', u'variable', u'btuscontinuos', u'electronic', u'ignition', u'-', u'prevents', u'dangerous', u'delayed', u'ignitionhome', u'depot', u'protection', u'plan:'])\n",
      "################\n",
      "words enlarge with desc and title\n",
      "Row(words_title=[u'dyna-glo', u'pro', u'125,000', u'btu', u'forced', u'air', u'lp', u'gas', u'portable', u'heater'], product_description=u'Dyna-Glo Pro portable gas forced air heaters offer a quick fire continuous spark ignition systems, coupled with adjustable height controls to insure you get the heat you need, when you need it, and where you want it. Fueled by liquid propane, this unit heats up to 3,100 sq. ft. 10 ft. Hose and regulator assembly are included (LP tank sold separately).70,000 - 125,000 BTUsHeats up to 3,100 sq. ft.Continuously variable BTUsContinuos electronic ignition - prevents dangerous delayed ignitionHome Depot Protection Plan:', id=1019, words=[u'dyna-glo', u'pro', u'125,000', u'btu', u'forced', u'air', u'lp', u'gas', u'portable', u'heater', u'dyna-glo', u'pro', u'portable', u'gas', u'forced', u'air', u'heaters', u'offer', u'a', u'quick', u'fire', u'continuous', u'spark', u'ignition', u'systems,', u'coupled', u'with', u'adjustable', u'height', u'controls', u'to', u'insure', u'you', u'get', u'the', u'heat', u'you', u'need,', u'when', u'you', u'need', u'it,', u'and', u'where', u'you', u'want', u'it.', u'fueled', u'by', u'liquid', u'propane,', u'this', u'unit', u'heats', u'up', u'to', u'3,100', u'sq.', u'ft.', u'10', u'ft.', u'hose', u'and', u'regulator', u'assembly', u'are', u'included', u'(lp', u'tank', u'sold', u'separately).70,000', u'-', u'125,000', u'btusheats', u'up', u'to', u'3,100', u'sq.', u'ft.continuously', u'variable', u'btuscontinuos', u'electronic', u'ignition', u'-', u'prevents', u'dangerous', u'delayed', u'ignitionhome', u'depot', u'protection', u'plan:'], relevance=1.0, attributes=u'Area Heated (Sq. Ft.) 3100. Bullet01 70,000 - 125,000 BTUs. Bullet02 Heats up to 3,100 sq. ft.. Bullet03 Continuously variable BTUs. Bullet04 Continuos electronic ignition - prevents dangerous delayed ignition. Certifications and Listings CSA Listed. Color Orange. Color Family Oranges / Peaches. Fuel rate (gallons/hour) 0. Fuel tank capacity (gallons) 0. Heat rating (BTU/hour) 125000. Heater Type Vented. Heating Product Type Gas Portable Heater. Heating Technology Type Convection. Ignition Type Continuous Spark. Indoor/Outdoor Indoor/Outdoor. Material Steel. MFG Brand Name Dyna-Glo Pro. Portable Heater Features Automatic Shutoff,Tip-Over Safety Switch,Wheels. Power/Fuel Type Propane. Product Depth (in.) 11.81. Product Height (in.) 16.14. Product Weight (lb.) 20.72. Product Width (in.) 25.59. Run time (hours) 12.', search_term=u'kerosene heater', words_desc=[u'dyna-glo', u'pro', u'portable', u'gas', u'forced', u'air', u'heaters', u'offer', u'a', u'quick', u'fire', u'continuous', u'spark', u'ignition', u'systems,', u'coupled', u'with', u'adjustable', u'height', u'controls', u'to', u'insure', u'you', u'get', u'the', u'heat', u'you', u'need,', u'when', u'you', u'need', u'it,', u'and', u'where', u'you', u'want', u'it.', u'fueled', u'by', u'liquid', u'propane,', u'this', u'unit', u'heats', u'up', u'to', u'3,100', u'sq.', u'ft.', u'10', u'ft.', u'hose', u'and', u'regulator', u'assembly', u'are', u'included', u'(lp', u'tank', u'sold', u'separately).70,000', u'-', u'125,000', u'btusheats', u'up', u'to', u'3,100', u'sq.', u'ft.continuously', u'variable', u'btuscontinuos', u'electronic', u'ignition', u'-', u'prevents', u'dangerous', u'delayed', u'ignitionhome', u'depot', u'protection', u'plan:'], product_uid=100170, product_title=u'Dyna-Glo Pro 125,000 BTU Forced Air LP Gas Portable Heater')\n",
      "################\n",
      "TERM frequencies:\n",
      "Row(words_title=[u'dyna-glo', u'pro', u'125,000', u'btu', u'forced', u'air', u'lp', u'gas', u'portable', u'heater'], product_description=u'Dyna-Glo Pro portable gas forced air heaters offer a quick fire continuous spark ignition systems, coupled with adjustable height controls to insure you get the heat you need, when you need it, and where you want it. Fueled by liquid propane, this unit heats up to 3,100 sq. ft. 10 ft. Hose and regulator assembly are included (LP tank sold separately).70,000 - 125,000 BTUsHeats up to 3,100 sq. ft.Continuously variable BTUsContinuos electronic ignition - prevents dangerous delayed ignitionHome Depot Protection Plan:', id=1019, words=[u'dyna-glo', u'pro', u'125,000', u'btu', u'forced', u'air', u'lp', u'gas', u'portable', u'heater', u'dyna-glo', u'pro', u'portable', u'gas', u'forced', u'air', u'heaters', u'offer', u'a', u'quick', u'fire', u'continuous', u'spark', u'ignition', u'systems,', u'coupled', u'with', u'adjustable', u'height', u'controls', u'to', u'insure', u'you', u'get', u'the', u'heat', u'you', u'need,', u'when', u'you', u'need', u'it,', u'and', u'where', u'you', u'want', u'it.', u'fueled', u'by', u'liquid', u'propane,', u'this', u'unit', u'heats', u'up', u'to', u'3,100', u'sq.', u'ft.', u'10', u'ft.', u'hose', u'and', u'regulator', u'assembly', u'are', u'included', u'(lp', u'tank', u'sold', u'separately).70,000', u'-', u'125,000', u'btusheats', u'up', u'to', u'3,100', u'sq.', u'ft.continuously', u'variable', u'btuscontinuos', u'electronic', u'ignition', u'-', u'prevents', u'dangerous', u'delayed', u'ignitionhome', u'depot', u'protection', u'plan:'], relevance=1.0, attributes=u'Area Heated (Sq. Ft.) 3100. Bullet01 70,000 - 125,000 BTUs. Bullet02 Heats up to 3,100 sq. ft.. Bullet03 Continuously variable BTUs. Bullet04 Continuos electronic ignition - prevents dangerous delayed ignition. Certifications and Listings CSA Listed. Color Orange. Color Family Oranges / Peaches. Fuel rate (gallons/hour) 0. Fuel tank capacity (gallons) 0. Heat rating (BTU/hour) 125000. Heater Type Vented. Heating Product Type Gas Portable Heater. Heating Technology Type Convection. Ignition Type Continuous Spark. Indoor/Outdoor Indoor/Outdoor. Material Steel. MFG Brand Name Dyna-Glo Pro. Portable Heater Features Automatic Shutoff,Tip-Over Safety Switch,Wheels. Power/Fuel Type Propane. Product Depth (in.) 11.81. Product Height (in.) 16.14. Product Weight (lb.) 20.72. Product Width (in.) 25.59. Run time (hours) 12.', search_term=u'kerosene heater', words_desc=[u'dyna-glo', u'pro', u'portable', u'gas', u'forced', u'air', u'heaters', u'offer', u'a', u'quick', u'fire', u'continuous', u'spark', u'ignition', u'systems,', u'coupled', u'with', u'adjustable', u'height', u'controls', u'to', u'insure', u'you', u'get', u'the', u'heat', u'you', u'need,', u'when', u'you', u'need', u'it,', u'and', u'where', u'you', u'want', u'it.', u'fueled', u'by', u'liquid', u'propane,', u'this', u'unit', u'heats', u'up', u'to', u'3,100', u'sq.', u'ft.', u'10', u'ft.', u'hose', u'and', u'regulator', u'assembly', u'are', u'included', u'(lp', u'tank', u'sold', u'separately).70,000', u'-', u'125,000', u'btusheats', u'up', u'to', u'3,100', u'sq.', u'ft.continuously', u'variable', u'btuscontinuos', u'electronic', u'ignition', u'-', u'prevents', u'dangerous', u'delayed', u'ignitionhome', u'depot', u'protection', u'plan:'], product_uid=100170, product_title=u'Dyna-Glo Pro 125,000 BTU Forced Air LP Gas Portable Heater', tf=SparseVector(262144, {7123: 1.0, 7838: 2.0, 8246: 1.0, 22046: 1.0, 22323: 1.0, 24980: 1.0, 26435: 1.0, 29509: 1.0, 38453: 2.0, 43583: 1.0, 45531: 2.0, 46299: 1.0, 47309: 1.0, 48804: 1.0, 49455: 1.0, 61013: 1.0, 67959: 2.0, 71920: 1.0, 76573: 2.0, 80740: 1.0, 81229: 1.0, 83161: 1.0, 85236: 1.0, 87419: 1.0, 91677: 2.0, 91799: 1.0, 99895: 1.0, 100736: 1.0, 102429: 1.0, 103838: 1.0, 105464: 1.0, 106157: 1.0, 106754: 1.0, 108510: 2.0, 108541: 1.0, 120885: 1.0, 121316: 1.0, 126466: 1.0, 126787: 2.0, 133143: 1.0, 133514: 1.0, 142657: 2.0, 144637: 1.0, 148260: 2.0, 148851: 1.0, 150447: 1.0, 159899: 1.0, 167122: 1.0, 179344: 2.0, 184075: 2.0, 188570: 2.0, 188822: 1.0, 190256: 1.0, 193098: 1.0, 195132: 1.0, 196522: 1.0, 203072: 1.0, 203758: 1.0, 205044: 3.0, 206361: 1.0, 209594: 1.0, 209649: 1.0, 211418: 1.0, 227410: 1.0, 228901: 1.0, 233720: 1.0, 234657: 1.0, 240647: 1.0, 249835: 2.0, 251323: 1.0, 252801: 4.0, 259523: 1.0}))\n",
      "################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF :\n",
      "Row(words_title=[u'dyna-glo', u'pro', u'125,000', u'btu', u'forced', u'air', u'lp', u'gas', u'portable', u'heater'], product_description=u'Dyna-Glo Pro portable gas forced air heaters offer a quick fire continuous spark ignition systems, coupled with adjustable height controls to insure you get the heat you need, when you need it, and where you want it. Fueled by liquid propane, this unit heats up to 3,100 sq. ft. 10 ft. Hose and regulator assembly are included (LP tank sold separately).70,000 - 125,000 BTUsHeats up to 3,100 sq. ft.Continuously variable BTUsContinuos electronic ignition - prevents dangerous delayed ignitionHome Depot Protection Plan:', id=1019, words=[u'dyna-glo', u'pro', u'125,000', u'btu', u'forced', u'air', u'lp', u'gas', u'portable', u'heater', u'dyna-glo', u'pro', u'portable', u'gas', u'forced', u'air', u'heaters', u'offer', u'a', u'quick', u'fire', u'continuous', u'spark', u'ignition', u'systems,', u'coupled', u'with', u'adjustable', u'height', u'controls', u'to', u'insure', u'you', u'get', u'the', u'heat', u'you', u'need,', u'when', u'you', u'need', u'it,', u'and', u'where', u'you', u'want', u'it.', u'fueled', u'by', u'liquid', u'propane,', u'this', u'unit', u'heats', u'up', u'to', u'3,100', u'sq.', u'ft.', u'10', u'ft.', u'hose', u'and', u'regulator', u'assembly', u'are', u'included', u'(lp', u'tank', u'sold', u'separately).70,000', u'-', u'125,000', u'btusheats', u'up', u'to', u'3,100', u'sq.', u'ft.continuously', u'variable', u'btuscontinuos', u'electronic', u'ignition', u'-', u'prevents', u'dangerous', u'delayed', u'ignitionhome', u'depot', u'protection', u'plan:'], relevance=1.0, attributes=u'Area Heated (Sq. Ft.) 3100. Bullet01 70,000 - 125,000 BTUs. Bullet02 Heats up to 3,100 sq. ft.. Bullet03 Continuously variable BTUs. Bullet04 Continuos electronic ignition - prevents dangerous delayed ignition. Certifications and Listings CSA Listed. Color Orange. Color Family Oranges / Peaches. Fuel rate (gallons/hour) 0. Fuel tank capacity (gallons) 0. Heat rating (BTU/hour) 125000. Heater Type Vented. Heating Product Type Gas Portable Heater. Heating Technology Type Convection. Ignition Type Continuous Spark. Indoor/Outdoor Indoor/Outdoor. Material Steel. MFG Brand Name Dyna-Glo Pro. Portable Heater Features Automatic Shutoff,Tip-Over Safety Switch,Wheels. Power/Fuel Type Propane. Product Depth (in.) 11.81. Product Height (in.) 16.14. Product Weight (lb.) 20.72. Product Width (in.) 25.59. Run time (hours) 12.', search_term=u'kerosene heater', words_desc=[u'dyna-glo', u'pro', u'portable', u'gas', u'forced', u'air', u'heaters', u'offer', u'a', u'quick', u'fire', u'continuous', u'spark', u'ignition', u'systems,', u'coupled', u'with', u'adjustable', u'height', u'controls', u'to', u'insure', u'you', u'get', u'the', u'heat', u'you', u'need,', u'when', u'you', u'need', u'it,', u'and', u'where', u'you', u'want', u'it.', u'fueled', u'by', u'liquid', u'propane,', u'this', u'unit', u'heats', u'up', u'to', u'3,100', u'sq.', u'ft.', u'10', u'ft.', u'hose', u'and', u'regulator', u'assembly', u'are', u'included', u'(lp', u'tank', u'sold', u'separately).70,000', u'-', u'125,000', u'btusheats', u'up', u'to', u'3,100', u'sq.', u'ft.continuously', u'variable', u'btuscontinuos', u'electronic', u'ignition', u'-', u'prevents', u'dangerous', u'delayed', u'ignitionhome', u'depot', u'protection', u'plan:'], product_uid=100170, product_title=u'Dyna-Glo Pro 125,000 BTU Forced Air LP Gas Portable Heater', tf=SparseVector(262144, {7123: 1.0, 7838: 2.0, 8246: 1.0, 22046: 1.0, 22323: 1.0, 24980: 1.0, 26435: 1.0, 29509: 1.0, 38453: 2.0, 43583: 1.0, 45531: 2.0, 46299: 1.0, 47309: 1.0, 48804: 1.0, 49455: 1.0, 61013: 1.0, 67959: 2.0, 71920: 1.0, 76573: 2.0, 80740: 1.0, 81229: 1.0, 83161: 1.0, 85236: 1.0, 87419: 1.0, 91677: 2.0, 91799: 1.0, 99895: 1.0, 100736: 1.0, 102429: 1.0, 103838: 1.0, 105464: 1.0, 106157: 1.0, 106754: 1.0, 108510: 2.0, 108541: 1.0, 120885: 1.0, 121316: 1.0, 126466: 1.0, 126787: 2.0, 133143: 1.0, 133514: 1.0, 142657: 2.0, 144637: 1.0, 148260: 2.0, 148851: 1.0, 150447: 1.0, 159899: 1.0, 167122: 1.0, 179344: 2.0, 184075: 2.0, 188570: 2.0, 188822: 1.0, 190256: 1.0, 193098: 1.0, 195132: 1.0, 196522: 1.0, 203072: 1.0, 203758: 1.0, 205044: 3.0, 206361: 1.0, 209594: 1.0, 209649: 1.0, 211418: 1.0, 227410: 1.0, 228901: 1.0, 233720: 1.0, 234657: 1.0, 240647: 1.0, 249835: 2.0, 251323: 1.0, 252801: 4.0, 259523: 1.0}), tf_idf=SparseVector(262144, {7123: 3.8836, 7838: 12.0285, 8246: 6.2855, 22046: 9.2668, 22323: 2.6204, 24980: 1.9784, 26435: 9.6033, 29509: 7.4751, 38453: 9.9101, 43583: 3.1862, 45531: 3.694, 46299: 2.8908, 47309: 2.1862, 48804: 4.4593, 49455: 7.5752, 61013: 3.782, 67959: 2.7547, 71920: 2.1202, 76573: 15.6909, 80740: 2.5829, 81229: 3.3975, 83161: 2.554, 85236: 5.2113, 87419: 4.4593, 91677: 0.0837, 91799: 4.154, 99895: 2.8332, 100736: 5.8468, 102429: 5.0307, 103838: 0.097, 105464: 3.1126, 106157: 7.7162, 106754: 5.4729, 108510: 13.3603, 108541: 0.6878, 120885: 3.0853, 121316: 1.8896, 126466: 0.2077, 126787: 8.1351, 133143: 1.2821, 133514: 9.6033, 142657: 17.4557, 144637: 5.6676, 148260: 8.9468, 148851: 2.8952, 150447: 7.8115, 159899: 4.126, 167122: 1.0528, 179344: 2.7847, 184075: 4.958, 188570: 4.9826, 188822: 4.7623, 190256: 4.0403, 193098: 3.5303, 195132: 5.499, 196522: 6.7819, 203072: 5.8946, 203758: 4.0763, 205044: 0.3594, 206361: 3.7862, 209594: 2.787, 209649: 3.9815, 211418: 3.3293, 227410: 0.1615, 228901: 6.2291, 233720: 6.2223, 234657: 5.971, 240647: 7.1523, 249835: 6.4362, 251323: 2.302, 252801: 4.678, 259523: 2.5418}))\n",
      "################\n",
      "NEW features column :\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o476.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 276.0 failed 1 times, most recent failure: Lost task 0.0 in stage 276.0 (TID 5182, localhost, executor driver): java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2768)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2765)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2765)\n\tat sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8895363ac462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0mfulldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"NEW features column :\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mfulldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"################\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \"\"\"\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \"\"\"\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o476.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 276.0 failed 1 times, most recent failure: Lost task 0.0 in stage 276.0 (TID 5182, localhost, executor driver): java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2768)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2765)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2765)\n\tat sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.SocketException: Socket is closed\n\tat java.net.Socket.getInputStream(Socket.java:903)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:151)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import Row\n",
    "from functools import partial\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import HTMLParser\n",
    "import re, collections\n",
    "\n",
    "#remove symbols and numbers and convert to lower case\n",
    "def words(s):\n",
    "\th = HTMLParser.HTMLParser()\n",
    "\ts=h.unescape(s)\n",
    "\ts=re.sub('[?!\\\\.,\\(\\)#\\\"\\']+',' ',s).strip()\n",
    "\ts=re.sub('[ ]+[0-9]+[/]+[0-9]+[ ]+',' ',s)\n",
    "\ts=re.sub('[ ]+[0-9]+[\\-]+[0-9]+[ ]+',' ',s)\n",
    "\ts=re.sub('[ ]+[0-9]+[\\.]+[0-9]+[ ]+',' ',s)\n",
    "\ts=re.sub('[ ]+[0-9]+[,]+[0-9]+[ ]+',' ',s)\n",
    "\ts=re.sub('[\\-]+',' ',s)\n",
    "\ts=re.sub('[0-9]+',' ',s)\n",
    "\ts=re.sub('([a-z]+)([A-Z]{1,1})([a-z]+)',r'\\1 \\2\\3',s)\n",
    "\ts=re.sub('\\s+',' ',s)\n",
    "\treturn re.findall('[a-z]+', s.lower())\n",
    "\t\n",
    "\n",
    "\n",
    "def fixEncoding(x):\n",
    "    # fix encoding in fields name and value\n",
    "    id = x['product_uid']\n",
    "    name = ''\n",
    "    if x['name'] is not None:\n",
    "        name = x['name'].encode(\"UTF-8\")\n",
    "    value = \"\"\n",
    "    if x['value'] is not None:\n",
    "        value = x['value'].encode(\"UTF-8\")\n",
    "    retVal = '%s %s.' % (name, value)\n",
    "    # return tuple instead of row\n",
    "    return (id, [retVal])\n",
    "\n",
    "\n",
    "def addFeatureLen(row):\n",
    "    vector = row['tf_idf']\n",
    "    size = vector.size\n",
    "    newVector = {}\n",
    "    for i, v in enumerate(vector.indices):\n",
    "        newVector[v] = vector.values[i]\n",
    "    newVector[size] = len(vector.indices)\n",
    "    size += 1\n",
    "    # we cannot change the input Row so we need to create a new one\n",
    "    data = row.asDict()\n",
    "    data['tf_idf'] = SparseVector(size, newVector)\n",
    "    # new Row object with specified NEW fields\n",
    "    newRow = Row(*data.keys())\n",
    "    # fill in the values for the fields\n",
    "    newRow = newRow(*data.values())\n",
    "    return newRow\n",
    "\n",
    "\n",
    "def cleanData(row, model):\n",
    "    # we are going to fix search term field\n",
    "    text = row['search_term'].split()\n",
    "    for i, v in enumerate(text):\n",
    "        text[i] = correct(v, model)\n",
    "    data = row.asDict()\n",
    "    # create new field for cleaned version\n",
    "    data['search_term2'] = text\n",
    "    newRow = Row(*data.keys())\n",
    "    newRow = newRow(*data.values())\n",
    "    return newRow\n",
    "\n",
    "\n",
    "def newFeatures(row):\n",
    "    vector = row['tf_idf']\n",
    "    data = row.asDict()\n",
    "    data['features'] = DenseVector([len(vector.indices), vector.values.min(), vector.values.max()])\n",
    "    newRow = Row(*data.keys())\n",
    "    newRow = newRow(*data.values())\n",
    "    return newRow\n",
    "\n",
    "\n",
    "def tfIdfAsNewFeatures(row):\n",
    "    vector = row['tf_idf']\n",
    "    data = row.asDict()    \n",
    "    data['features'] = DenseVector([len(vector.indices), vector.values.min(), vector.values.max(), vector.values.mean()])\n",
    "    newRow = Row(*data.keys())\n",
    "    newRow = newRow(*data.values())\n",
    "    return newRow\n",
    "\n",
    "def tfIdfAsNewFeaturesBis(row):\n",
    "    vector = row['tf_idf']\n",
    "    data = row.asDict()    \n",
    "    data['features'] = DenseVector(vector.toArray())\n",
    "    newRow = Row(*data.keys())\n",
    "    newRow = newRow(*data.values())\n",
    "    return newRow\n",
    "\n",
    "def enlargeToken(row):\n",
    "    vectorT = row['words_title']\n",
    "    vectorD = row['words_desc']\n",
    "    data = row.asDict()\n",
    "    data['words'] = vectorT + vectorD\n",
    "    newRow = Row(*data.keys())\n",
    "    newRow = newRow(*data.values())\n",
    "    return newRow\n",
    "\n",
    "def enlargeTokenAndClean(row):\n",
    "    vectorT = row['words_title']\n",
    "    vectorD = row['words_desc']\n",
    "    data = row.asDict()\n",
    "    data['words'] = vectorT + vectorD\n",
    "    #w=[]\n",
    "    #for word in data['words']:\n",
    "        #w += words(word)\n",
    "    #data['wordsF'] = w\n",
    "    newRow = Row(*data.keys())\n",
    "    newRow = newRow(*data.values())\n",
    "    return newRow\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "sqlContext = HiveContext(sc)\n",
    "print \"###############\"\n",
    "# READ data\n",
    "data = sqlContext.read.format(\"com.databricks.spark.csv\").\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    option(\"inferSchema\", \"true\").\\\n",
    "    load(\"train.csv\").repartition(100)\n",
    "print \"data loaded - head:\"\n",
    "print data.head()\n",
    "print \"################\"\n",
    "\n",
    "attributes = sqlContext.read.format(\"com.databricks.spark.csv\").\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    option(\"inferSchema\", \"true\").\\\n",
    "    load(\"attributes.csv\").repartition(100)\n",
    "\n",
    "print \"attributes loaded - head:\"\n",
    "print attributes.head()\n",
    "print \"################\"\n",
    "\n",
    "product_description = sqlContext.read.format(\"com.databricks.spark.csv\").\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    option(\"inferSchema\", \"true\").\\\n",
    "    load(\"product_descriptions.csv\").repartition(100)\n",
    "\n",
    "print \"description loaded - head:\"\n",
    "print product_description.head()\n",
    "print \"################\"\n",
    "\n",
    "\n",
    "# attributes: 0-N lines per product\n",
    "# Step 1 : fix encoding and get data as an RDD (id,\"<attribute name> <value>\")\n",
    "attRDD = attributes.rdd.map(fixEncoding)\n",
    "print \"new RDD:\"\n",
    "print attRDD.first()\n",
    "print \"################\"\n",
    "\n",
    "# Step 2 : group attributes by product id\n",
    "attAG = attRDD.reduceByKey(\n",
    "    lambda x, y: x + y).map(lambda x: (x[0], ' '.join(x[1])))\n",
    "print \"Aggregated by product_id:\"\n",
    "print attAG.first()\n",
    "print \"################\"\n",
    "\n",
    "# Step 3 create new dataframe from aggregated attributes\n",
    "atrDF = sqlContext.createDataFrame(attAG, [\"product_uid\", \"attributes\"])\n",
    "print \"New dataframe from aggregated attributes:\"\n",
    "print atrDF.head()\n",
    "print \"################\"\n",
    "\n",
    "# Step 4 join data with attribute\n",
    "\n",
    "withAttdata = data.join(atrDF, ['product_uid'], 'left_outer')\n",
    "print \"Joined Data:\"\n",
    "print withAttdata.head()\n",
    "#Row(product_uid=100501, id=2847, product_title=u'Ring Wireless Video Door Bell', search_term=u'door bell', relevance=3.0, attributes=u\"Adjustable Volume Yes. Bullet04 Multiple faceplate finishes helping you match your current door hardware. Mechanical Bell No. Product Width (in.) 2.4. Bullet03 Built-in motion sensors detect movement up to 30 ft. allowing you to know what is going on outside of your home. Multiple Songs No. Digital Bell No. Product Height (in.) 5. Bullet02 Compatible with all iOS and android Smartphone and tablets. Door Chime Kit Type Wired With Contacts. Number of Sounds 1. Zone-specific Sounds No. Bell Button Color Family Gray. Certifications and Listings No Certifications or Listings. Product Depth (in.) .9. Electrical Product Type Door Chime Kit. Transformer Not Included. Bullet01 See and speak with visitors using your Smartphone or tablet, whether you're upstairs or across town. Door Bell Or Intercom Type Door Bells. Number of Buttons Included 2. Westminster Bell No. Bell Wire Required Wireless. Bullet05 Connect to current doorbell wiring or utilize internal battery for convenience. MFG Brand Name Ring. Style Contemporary.\")\n",
    "print \"################\"\n",
    "\n",
    "# Step 5 join data with description\n",
    "print \"new RDD:\"\n",
    "print product_description.first()\n",
    "print \"################\"\n",
    "\n",
    "fulldata = withAttdata.join(product_description, ['product_uid'], 'left_outer')\n",
    "print \"Joined Data:\"\n",
    "print fulldata.head()\n",
    "print \"################\"\n",
    "\n",
    "\n",
    "# TF-IDF features\n",
    "# Step 1: split text field into words\n",
    "tokenizer = Tokenizer(inputCol=\"product_title\", outputCol=\"words_title\")\n",
    "fulldata = tokenizer.transform(fulldata)\n",
    "print \"Tokenized Title:\"\n",
    "print fulldata.head()\n",
    "print \"################\"\n",
    "\n",
    "# Step 1 Prim: split text field into words\n",
    "tokenizer = Tokenizer(inputCol=\"product_description\", outputCol=\"words_desc\")\n",
    "fulldata = tokenizer.transform(fulldata)\n",
    "print \"Tokenized Description:\"\n",
    "print fulldata.head()\n",
    "print \"################\"\n",
    "\n",
    "#Merge product with words\n",
    "\n",
    "fulldata = sqlContext.createDataFrame(fulldata.rdd.map((enlargeTokenAndClean)))                      \n",
    "print \"words enlarge with desc and title\"\n",
    "print fulldata.head()\n",
    "print \"################\"                                    \n",
    "\n",
    "# Step 2: compute term frequencies\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"tf\")\n",
    "fulldata = hashingTF.transform(fulldata)\n",
    "print \"TERM frequencies:\"\n",
    "print fulldata.head()\n",
    "print \"################\"\n",
    "# Step 3: compute inverse document frequencies\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tf_idf\")\n",
    "idfModel = idf.fit(fulldata)\n",
    "fulldata = idfModel.transform(fulldata)\n",
    "print \"IDF :\"\n",
    "print fulldata.head()\n",
    "print \"################\"\n",
    "\n",
    "# Step 4 new features column / rename old\n",
    "fulldata = sqlContext.createDataFrame(fulldata.rdd.map(addFeatureLen))\n",
    "fulldata = sqlContext.createDataFrame(fulldata.rdd.map(newFeatures))\n",
    "print \"NEW features column :\"\n",
    "print fulldata.head()\n",
    "print \"################\"\n",
    "\n",
    "\n",
    "# Step 5: ALTERNATIVE ->ADD column with number of terms as another feature\n",
    "#fulldata = sqlContext.createDataFrame(fulldata.rdd.map(\n",
    " #   addFeatureLen))  # add an extra column to tf features\n",
    "#fulldata = fulldata.withColumnRenamed('tf_idf', 'tf_idf_plus')\n",
    "#print \"ADDED a column and renamed :\"\n",
    "#print fulldata.head()\n",
    "#print \"################\"\n",
    "\n",
    "\n",
    "# create NEW features & train and evaluate regression model\n",
    "# Step 1: create features\n",
    "fulldata = fulldata.withColumnRenamed(\n",
    "    'relevance', 'label').select(['label', 'features'])\n",
    "print \"TRAIN - ADDED a column and renamed :\"\n",
    "print fulldata.head()\n",
    "print \"################\"\n",
    "\n",
    "\n",
    "# Simple evaluation : train and test split\n",
    "(train, test) = fulldata.rdd.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Initialize regresion model\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(sqlContext.createDataFrame(train))\n",
    "\n",
    "# Apply model to test data\n",
    "result = lrModel.transform(sqlContext.createDataFrame(test))\n",
    "# Compute mean squared error metric\n",
    "MSE = result.rdd.map(lambda r: (r['label'] - r['prediction'])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Mean Squared Error = 0.281658352253  (empty) ????????  c'est pas normal\n",
    "Mean Squared Error = 0.284223501857  (title seul)\n",
    "Mean Squared Error = 0.282793946361  (desc seul)\n",
    "Mean Squared Error = 0.280816931591 (title + desc)\n",
    "Mean Squared Error = 0.283601709652 (min+max+mean+len) c'est normal car la moyenne bruite la regression\n",
    "Mean Squared Error = 0.286859147648 (netoyage ponctu) ???? \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
